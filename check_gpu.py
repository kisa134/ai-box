#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ GPU –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã
"""

import torch
import GPUtil
import psutil
import subprocess
import sys

def check_gpu():
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å GPU"""
    print("üéÆ –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU")
    print("=" * 40)
    
    # 1. PyTorch GPU
    print("üìä PyTorch GPU:")
    if torch.cuda.is_available():
        print(f"‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞")
        print(f"   –í–µ—Ä—Å–∏—è CUDA: {torch.version.cuda}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
        
        # –¢–µ–∫—É—â–∏–π GPU
        current_device = torch.cuda.current_device()
        print(f"   –¢–µ–∫—É—â–∏–π GPU: {current_device}")
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        print(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ VRAM: {allocated:.2f}GB")
        print(f"   –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ VRAM: {cached:.2f}GB")
        
    else:
        print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
    
    print()
    
    # 2. GPUtil
    print("üìä GPUtil GPU:")
    try:
        gpus = GPUtil.getGPUs()
        if gpus:
            for i, gpu in enumerate(gpus):
                print(f"   GPU {i}: {gpu.name}")
                print(f"   VRAM: {gpu.memoryUsed}MB / {gpu.memoryTotal}MB")
                print(f"   –ó–∞–≥—Ä—É–∑–∫–∞: {gpu.load * 100:.1f}%")
                print(f"   –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {gpu.temperature}¬∞C")
        else:
            print("‚ùå GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ GPUtil: {e}")
    
    print()
    
    # 3. nvidia-smi
    print("üìä nvidia-smi:")
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ nvidia-smi —Ä–∞–±–æ—Ç–∞–µ—Ç")
            lines = result.stdout.split('\n')
            for line in lines[:10]:  # –ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫
                if line.strip():
                    print(f"   {line}")
        else:
            print("‚ùå nvidia-smi –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ nvidia-smi: {e}")
    
    print()
    
    # 4. –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
    print("üìä –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã:")
    memory = psutil.virtual_memory()
    print(f"   RAM: {memory.used / 1024**3:.1f}GB / {memory.total / 1024**3:.1f}GB ({memory.percent:.1f}%)")
    
    cpu_percent = psutil.cpu_percent(interval=1)
    print(f"   CPU: {cpu_percent:.1f}%")
    
    print()
    
    # 5. –¢–µ—Å—Ç GPU
    print("üß™ –¢–µ—Å—Ç GPU:")
    if torch.cuda.is_available():
        try:
            # –°–æ–∑–¥–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä –Ω–∞ GPU
            device = torch.device('cuda')
            x = torch.randn(1000, 1000).to(device)
            y = torch.randn(1000, 1000).to(device)
            
            # –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ
            start_time = torch.cuda.Event(enable_timing=True)
            end_time = torch.cuda.Event(enable_timing=True)
            
            start_time.record()
            z = torch.mm(x, y)
            end_time.record()
            
            torch.cuda.synchronize()
            elapsed_time = start_time.elapsed_time(end_time)
            
            print(f"‚úÖ GPU —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω")
            print(f"   –í—Ä–µ–º—è –º–∞—Ç—Ä–∏—á–Ω–æ–≥–æ —É–º–Ω–æ–∂–µ–Ω–∏—è: {elapsed_time:.2f}ms")
            print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {z.shape}")
            
            # –û—á–∏—Å—Ç–∏—Ç—å –ø–∞–º—è—Ç—å
            del x, y, z
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ GPU —Ç–µ—Å—Ç–∞: {e}")
    else:
        print("‚ùå GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")

def check_ollama_gpu():
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –≤ Ollama"""
    print("\nüîß –ü—Ä–æ–≤–µ—Ä–∫–∞ Ollama GPU:")
    print("=" * 40)
    
    try:
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å Ollama
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ Ollama —Ä–∞–±–æ—Ç–∞–µ—Ç")
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å—ã Ollama
            ollama_processes = []
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    if 'ollama' in proc.info['name'].lower():
                        ollama_processes.append(proc.info)
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            
            if ollama_processes:
                print(f"üìã –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ Ollama: {len(ollama_processes)}")
                for proc in ollama_processes:
                    print(f"   PID {proc['pid']}: {proc['name']}")
            else:
                print("‚ö†Ô∏è –ü—Ä–æ—Ü–µ—Å—Å—ã Ollama –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                
        else:
            print("‚ùå Ollama –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Ollama: {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã")
    print("=" * 60)
    
    check_gpu()
    check_ollama_gpu()
    
    print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

if __name__ == "__main__":
    main() 