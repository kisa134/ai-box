# üöÄ Ollama Setup –¥–ª—è AIbox

## üìã –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
- **GPU**: RTX 4090 (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è), RTX 4080, RTX 4070 –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∏
- **VRAM**: –ú–∏–Ω–∏–º—É–º 8GB, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 24GB+
- **RAM**: 16GB+, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 32GB+
- **OS**: Windows 10/11, Linux, macOS

### –ü—Ä–æ–≥—Ä–∞–º–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
- Python 3.9+
- Ollama (–ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è)
- CUDA 11.8+ (–¥–ª—è GPU —É—Å–∫–æ—Ä–µ–Ω–∏—è)

## üîß –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama

### Windows
```bash
# –°–∫–∞—á–∞—Ç—å –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Ollama
# https://ollama.ai/download

# –ò–ª–∏ —á–µ—Ä–µ–∑ winget
winget install Ollama.Ollama

# –ó–∞–ø—É—Å—Ç–∏—Ç—å Ollama
ollama serve
```

### Linux/macOS
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞
curl -fsSL https://ollama.ai/install.sh | sh

# –ó–∞–ø—É—Å–∫
ollama serve
```

## üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π

### –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
```bash
# Reasoning –º–æ–¥–µ–ª–∏
ollama pull mistral:latest      # 8GB VRAM
ollama pull llama3:latest       # 16GB VRAM
ollama pull phi3:latest         # 4GB VRAM (–¥–ª—è —Ç–µ—Å—Ç–æ–≤)

# –ú–æ—â–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å 24GB+ VRAM)
ollama pull mixtral:latest      # 24GB VRAM
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
```bash
# –°–ø–∏—Å–æ–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
ollama list

# –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏
ollama run mistral:latest "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"
```

## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ AIbox

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
```bash
pip install -r requirements.txt
```

### 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
```bash
# –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
cp env_example.txt .env

# –û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å .env
LLM_TYPE=ollama
OLLAMA_BASE_URL=http://localhost:11434
```

### 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
```bash
# –¢–µ—Å—Ç Ollama –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
python test_ollama_integration.py

# –¢–µ—Å—Ç –∞–≥–µ–Ω—Ç–∞ —Å Ollama
python test_llm.py
```

## üéØ –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π

### Reasoning (–õ–æ–≥–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ)
- **Mistral**: –ë—ã—Å—Ç—Ä–æ–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ
- **Mixtral**: –ú–æ—â–Ω–æ–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ  
- **Llama3**: –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ

### Reflection (–ì–ª—É–±–æ–∫–∞—è —Ä–µ—Ñ–ª–µ–∫—Å–∏—è)
- **Mistral**: –ì–ª—É–±–æ–∫–∞—è —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—è
- **Mixtral**: –ú–æ—â–Ω–∞—è —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—è

### Creative (–¢–≤–æ—Ä—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏)
- **Mistral**: –¢–≤–æ—Ä—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ
- **Llama3**: –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ

### Fast (–ë—ã—Å—Ç—Ä–æ–µ –º—ã—à–ª–µ–Ω–∏–µ)
- **Phi3**: –ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã
- **Mistral**: –ë—ã—Å—Ç—Ä–æ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ

### Subconscious (–ü–æ–¥—Å–æ–∑–Ω–∞–Ω–∏–µ)
- **Mistral**: –ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ
- **Llama3**: –ì–ª—É–±–∏–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ

## üîç –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤

### –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
```bash
# Windows
nvidia-smi

# Linux
nvidia-smi
watch -n 1 nvidia-smi
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ Ollama
```bash
# –°—Ç–∞—Ç—É—Å Ollama
curl http://localhost:11434/api/tags

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
ollama ps
```

## üöÄ –ó–∞–ø—É—Å–∫ AIbox —Å Ollama

### –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
```bash
python run_agent.py
```

### –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
```bash
streamlit run streamlit_app.py
```

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
```bash
# –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç
python test_ollama_integration.py

# –¢–µ—Å—Ç —Å–æ–∑–Ω–∞–Ω–∏—è
python test_consciousness.py
```

## üìä –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –î–ª—è RTX 4090 (24GB VRAM)
```yaml
# ollama_config.yaml
models:
  reasoning:
    mixtral:latest:  # –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å
    mistral:latest:   # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
  reflection:
    mixtral:latest:   # –ì–ª—É–±–æ–∫–∞—è —Ä–µ—Ñ–ª–µ–∫—Å–∏—è
  creative:
    mistral:latest:   # –¢–≤–æ—Ä—á–µ—Å—Ç–≤–æ
```

### –î–ª—è RTX 4080 (16GB VRAM)
```yaml
models:
  reasoning:
    llama3:latest:    # –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å
    mistral:latest:   # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
  reflection:
    mistral:latest:   # –†–µ—Ñ–ª–µ–∫—Å–∏—è
  creative:
    mistral:latest:   # –¢–≤–æ—Ä—á–µ—Å—Ç–≤–æ
```

### –î–ª—è RTX 4070 (12GB VRAM)
```yaml
models:
  reasoning:
    mistral:latest:   # –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å
    phi3:latest:      # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
  reflection:
    mistral:latest:   # –†–µ—Ñ–ª–µ–∫—Å–∏—è
  creative:
    mistral:latest:   # –¢–≤–æ—Ä—á–µ—Å—Ç–≤–æ
```

## üîß Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞: Ollama –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å
ollama serve

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å
pkill ollama
ollama serve
```

### –ü—Ä–æ–±–ª–µ–º–∞: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ VRAM
```bash
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å
ollama pull phi3:latest

# –ò–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –±–µ–∑ GPU
OLLAMA_HOST=0.0.0.0:11434 ollama serve
```

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–µ–¥–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å GPU –∑–∞–≥—Ä—É–∑–∫—É
nvidia-smi

# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
# –í ollama_config.yaml —É–º–µ–Ω—å—à–∏—Ç—å max_tokens
```

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
curl https://ollama.ai

# –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å
ollama pull mistral:latest
```

## üéØ –ë—É–¥—É—â–∏–µ –º–æ–¥–µ–ª–∏

### –ü–ª–∞–Ω–∏—Ä—É–µ–º—ã–µ –∫ –¥–æ–±–∞–≤–ª–µ–Ω–∏—é
- **Llama 4**: –ù–æ–≤–µ–π—à–∞—è –º–æ–¥–µ–ª—å –æ—Ç Meta
- **Mistral Large**: –ú–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å –æ—Ç Mistral AI
- **DeepSeek**: –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
- **Qwen 3**: –ö–∏—Ç–∞–π—Å–∫–∞—è –º–æ–¥–µ–ª—å –æ—Ç Alibaba
- **Gemma 2**: Google –º–æ–¥–µ–ª—å
- **Mamba**: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ State Space

### –ö–æ–º–∞–Ω–¥—ã –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
```bash
# –ö–æ–≥–¥–∞ –º–æ–¥–µ–ª–∏ —Å—Ç–∞–Ω—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã
ollama pull llama4:latest
ollama pull mistral-large:latest
ollama pull deepseek:latest
ollama pull qwen3:latest
ollama pull gemma2:latest
ollama pull mamba:latest
```

## üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏

### –õ–æ–≥–∏ reasoning
```bash
# –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤
tail -f reasoning_logs.jsonl

# –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤
python -c "
import json
with open('reasoning_logs.jsonl', 'r') as f:
    logs = [json.loads(line) for line in f]
print(f'–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(logs)}')
print(f'–£—Å–ø–µ—à–Ω—ã—Ö: {sum(1 for log in logs if log[\"response\"][\"success\"])}')
"
```

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤
```bash
# –°–∫—Ä–∏–ø—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
python -c "
from core.ollama_module import ResourceMonitor
monitor = ResourceMonitor()
resources = monitor.get_system_resources()
print(f'CPU: {resources[\"cpu_percent\"]:.1f}%')
print(f'RAM: {resources[\"ram_percent\"]:.1f}%')
if 'gpu' in resources:
    gpu = resources['gpu']
    print(f'GPU: {gpu[\"gpu_memory_percent\"]:.1f}%')
"
```

## üéâ –ì–æ—Ç–æ–≤–æ!

–¢–µ–ø–µ—Ä—å AIbox –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ —Å Ollama! 

### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:
1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç—ã: `python test_ollama_integration.py`
2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–≥–µ–Ω—Ç–∞: `streamlit run streamlit_app.py`
3. –ù–∞—á–Ω–∏—Ç–µ –¥–∏–∞–ª–æ–≥ –≤ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ
4. –ò–∑—É—á–∏—Ç–µ –ª–æ–≥–∏ reasoning –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞–±–æ—Ç—ã

### –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:
```bash
# –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
python test_ollama_integration.py

# –ó–∞–ø—É—Å–∫ –∞–≥–µ–Ω—Ç–∞
streamlit run streamlit_app.py

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤
watch -n 1 nvidia-smi

# –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤
tail -f reasoning_logs.jsonl
```

üöÄ **AIbox —Å Ollama –≥–æ—Ç–æ–≤ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–æ–∑–Ω–∞–Ω–∏—è!** 